# -*- coding: utf-8 -*-
"""Wardrobe_Wizard_Alexnet_Darter

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LW18gelJDEHGanZsYtbeAhaSYfz5FnFa

Wardrobe Wizard Project - MAP 2192 - Holly Darter

Install and import necessary libraries and tools for experiment tracking, PDF manipulation, image conversion, and PyTorch visualization
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install wandb
# !apt-get install poppler-utils
# !pip install pdf2image
# !pip install flashtorch
# import requests
# from pdf2image import convert_from_path
# import matplotlib.pyplot as plt
# import numpy as np
# import torch
# import requests
# from torchvision import *
# from torchvision.models import *
# import wandb as wb

# Determine the device (GPU if available, otherwise CPU)
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Function to create a GPU tensor with gradient tracking
def GPU(data):
    return torch.tensor(data, requires_grad=True, dtype=torch.float, device=device)

# Function to create a GPU tensor without gradient tracking
def GPU_data(data):
    return torch.tensor(data, requires_grad=False, dtype=torch.float, device=device)

# Function to plot a grayscale image
def plot(x):
    fig, ax = plt.subplots()
    im = ax.imshow(x, cmap = 'gray')
    ax.axis('off')
    fig.set_size_inches(5, 5)
    plt.show()

# Function to retrieve Google Slides PDF link from the presentation URL
def get_google_slide(url):
    url_head = "https://docs.google.com/presentation/d/"
    url_body = url.split('/')[5]
    page_id = url.split('.')[-1]
    return url_head + url_body + "/export/pdf?id=" + url_body + "&pageid=" + page_id

# Function to download and convert slides from a Google Slides presentation URL
def get_slides(url):
    url = get_google_slide(url)
    r = requests.get(url, allow_redirects=True)
    open('file.pdf', 'wb').write(r.content)
    images = convert_from_path('file.pdf', 500)
    return images

# Function to load and preprocess an image with PyTorch transformations
def load(image, size=224):
    means = [0.485, 0.456, 0.406]
    stds = [0.229, 0.224, 0.225]
    transform = transforms.Compose([
        transforms.Resize(size),
        transforms.CenterCrop(size),
        transforms.ToTensor(),
        transforms.Normalize(means, stds)
    ])
    tensor = transform(image).unsqueeze(0).to(device)
    tensor.requires_grad = True
    return tensor

# Retrieve labels from a JSON file hosted on Amazon S3 and convert them to a dictionary
labels = {int(key):value for (key, value) in requests.get('https://s3.amazonaws.com/mlpipes/pytorch-quick-start/labels.json').json().items()}

# Instantiate an AlexNet model with pre-trained weights and move it to the specified device (GPU or CPU)
model = alexnet(weights='DEFAULT').to(device)

# Set the model to evaluation mode, which disables dropout layers and ensures consistent inference behavior
model.eval();

"""Loading my Google Slides Data Deck"""

url = "https://docs.google.com/presentation/d/1w-UwMzHvUYb3D-X_rpMsPCt_y8ej8LpBzYM9ql7BFmc/edit#slide=id.p"

# Initialize an empty list to store images
images = []

# Iterate over images obtained from the Google Slides presentation using the get_slides function
for image in get_slides(url):
  # Plot each image using the plot function
    plot(image)
  # Preprocess and load each image into a PyTorch tensor using the load function
    images.append(load(image))

# Convert the list of PyTorch tensors into a single tensor by vertically stacking them
images = torch.vstack(images)

"""Obtain the shape of the images tensor, representing the dimensions (batch size, channels, height, width)"""

images.shape

"""Perform forward pass of the images through the pre-trained AlexNet model to obtain predictions"""

model(images)

"""Generate predictions (y) by passing the preprocessed images through the pre-trained AlexNet model"""

y = model(images)

"""Obtain the shape of the predictions tensor (y), representing the dimensions (batch size, number of classes)"""

y.shape

"""Extract the class indices with the highest predicted probability for each image"""

guesses = torch.argmax(y, 1).cpu().numpy()

"""Print the corresponding labels for each predicted class index in the 'guesses' list"""

for i in list(guesses):
    print(labels[i])

"""Initialize a NumPy array 'Y' of length 50 with all zeros, and set values from index"""

Y = np.zeros(50,)
Y[25:] = 1

"""Extract the NumPy array 'X' from the PyTorch tensor 'y' after detaching it from the computation graph and moving it to CPU"""

X = y.detach().cpu().numpy()

"""Obtain the shape of the NumPy array 'X', representing the dimensions (batch size, number of classes)"""

X.shape

"""Plot the values of the first element in the NumPy array 'X' using dots"""

plt.plot(X[0],'.')

"""Create a histogram of the values in the first element of the NumPy array 'X'"""

plt.hist(X[0])

"""Convert NumPy arrays 'X' and 'Y' to GPU tensors without gradient tracking"""

X = GPU_data(X)
Y = GPU_data(Y)

"""Define a softmax function to compute the softmax probabilities for each class"""

def softmax(x):
    s1 = torch.exp(x - torch.max(x,1)[0][:,None])
    s = s1 / s1.sum(1)[:,None]
    return s

"""Define a cross-entropy loss function for classification"""

def cross_entropy(outputs, labels):
    # Compute the negative log likelihood of the true class probabilities
    return -torch.sum(softmax(outputs).log()[range(outputs.size()[0]), labels.long()])/outputs.size()[0]

"""Define a function to generate truncated normal random numbers"""

def randn_trunc(s): #Truncated Normal Random Numbers
  # Set mean and standard deviation for the truncated normal distribution
    mu = 0
    sigma = 0.1

    # Create a truncated normal distribution with specified bounds and parameters
    R = stats.truncnorm((-2*sigma - mu) / sigma, (2*sigma - mu) / sigma, loc=mu, scale=sigma)

    # Generate 's' random numbers from the truncated normal distribution
    return R.rvs(s)

"""Define a function to generate random numbers from a truncated normal distribution"""

def Truncated_Normal(size):

  # Generate random values 'u1' and 'u2' within specified bounds
    u1 = torch.rand(size)*(1-np.exp(-2)) + np.exp(-2)
    u2 = torch.rand(size)

    # Compute random values 'z' from a truncated normal distribution
    z  = torch.sqrt(-2*torch.log(u1)) * torch.cos(2*np.pi*u2)

    return z

"""Define a function to calculate accuracy between model predictions ('out') and ground truth labels ('y')"""

def acc(out,y):
  # Use torch.no_grad() to disable gradient computation during accuracy calculation
    with torch.no_grad():
      # Calculate accuracy by comparing predicted and true labels, then normalizing by the batch size
        return (torch.sum(torch.max(out,1)[1] == y).item())/y.shape[0]

"""Define a function to get a batch of data based on the specified mode (train or test)"""

def get_batch(mode):
  # Retrieve batch size 'b' from the configuration (assumed to be defined elsewhere as 'c.b')
    b = c.b
    if mode == "train":
       # Randomly select a batch of training data and labels
        r = np.random.randint(X.shape[0]-b)
        x = X[r:r+b,:]
        y = Y[r:r+b]
    elif mode == "test":
      # Randomly select a batch of testing data and labels
        r = np.random.randint(X_test.shape[0]-b)
        x = X_test[r:r+b,:]
        y = Y_test[r:r+b]
    return x,y

"""Define a simple linear model that performs matrix multiplication between input 'x' and weights 'w[0]'"""

def model(x,w):

    return x@w[0]

# Define a function to generate and log training accuracy plots using Weights and Biases (WandB)
def make_plots():

  # Calculate training accuracy using the 'model' function
    acc_train = acc(model(x,w),y)

    # Log the training accuracy using Weights and Biases (WandB)
    wb.log({"acc_train": acc_train})

# Initialize Weights and Biases (WandB) for experiment tracking and configuration
wb.init(project="Linear_Model_Photo_1");
c = wb.config

# Set configuration parameters for learning rate, batch size, and number of epochs
c.h = 0.002
c.b = 32
c.epochs =  50000

# Initialize weights using truncated normal random numbers and move to GPU
w = [GPU(Truncated_Normal((1000,2)))]

# Set up the Adam optimizer with the specified learning rate
optimizer = torch.optim.Adam(w, lr=c.h)

# Training loop for the specified number of epochs
for i in range(c.epochs):
  # Get a batch of training data and labels
    x,y = get_batch('train')

# Calculate the cross-entropy loss using the current model and batch
    loss = cross_entropy(softmax(model(x,w)),y)

  # Zero the gradients, perform backward pass, and update the weights
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

 # Log the current loss using Weights and Biases (WandB)
    wb.log({"loss": loss})

  # Generate and log training accuracy plots
    make_plots()

"""Finish and close the Weights and Biases (WandB) experiment"""

wb.finish()